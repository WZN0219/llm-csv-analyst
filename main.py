import re
import pandas as pd
from llm_client import LLMClient
from code_executor import CodeExecutor
from prompts import get_initial_prompt, get_error_correction_prompt, get_explanation_prompt

def extract_code(text):
    """ä» Markdown ä¸­æå– Python ä»£ç """
    match = re.search(r"```python(.*?)```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text

def main():
    print("=== å¤§æ¨¡å‹ CSV æ•°æ®åˆ†æ Agent (Powered by Qwen) ===")
    
    # 1. è¯»å–æ•°æ®
    csv_path = "data/test.csv"
    try:
        df = pd.read_csv(csv_path)
        print(f"æˆåŠŸåŠ è½½æ•°æ®: {csv_path} | è¡Œæ•°: {len(df)}")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶åœ¨ data ç›®å½•ä¸‹ã€‚")
        return

    # 2. åˆå§‹åŒ–æ¨¡å—
    llm = LLMClient()
    executor = CodeExecutor(df) # æŠŠ df ä¼ ç»™æ‰§è¡Œå™¨

    # 3. æ„å»ºåˆå§‹ Prompt (åŒ…å«æ•°æ®æ‘˜è¦)
    df_head = df.head().to_markdown(index=False)
    df_info = str(df.dtypes)
    system_prompt = get_initial_prompt(df_head, df_info)
    llm.initialize_system(system_prompt)
    
    print("ç³»ç»Ÿå°±ç»ªã€‚è¯·å¼€å§‹æé—® (è¾“å…¥ 'exit' é€€å‡º)")
    
    # 4. ä¸»å¾ªç¯ (å¤šè½®å¯¹è¯)
    while True:
        user_query = input("\nğŸ‘¤ ç”¨æˆ·: ")
        if user_query.lower() in ['exit', 'quit']:
            break
        
        print("ğŸ¤– Agent æ€è€ƒä¸­...")
        
        # --- æ­¥éª¤ A: ç”Ÿæˆä»£ç  ---
        response = llm.get_response(f"ç”¨æˆ·éœ€æ±‚: {user_query}")
        code = extract_code(response)
        
        print(f"\n--- ç”Ÿæˆçš„ä»£ç  ---\n{code}\n------------------")
        
        # --- æ­¥éª¤ B: æ‰§è¡Œä¸çº é”™ ---
        max_retries = 3
        execution_success = False
        execution_output = ""
        
        for i in range(max_retries):
            # æ‰§è¡Œä»£ç 
            success, output = executor.execute(code)
            
            if success:
                execution_success = True
                execution_output = output
                print(f"âœ… æ‰§è¡ŒæˆåŠŸã€‚è¾“å‡ºç»“æœ:\n{output[:500]}..." if len(output)>500 else f"âœ… æ‰§è¡ŒæˆåŠŸã€‚è¾“å‡ºç»“æœ:\n{output}")
                break
            else:
                print(f"âŒ æ‰§è¡ŒæŠ¥é”™ (å°è¯• {i+1}/{max_retries}):\n{output.splitlines()[-1]}") # åªæ‰“å°æœ€åä¸€è¡ŒæŠ¥é”™
                
                # è§¦å‘çº é”™å¾ªç¯
                correction_prompt = get_error_correction_prompt(output)
                print("ğŸ”„ æ­£åœ¨è‡ªæˆ‘ä¿®æ­£ä»£ç ...")
                response = llm.get_response(correction_prompt, is_error_feedback=True)
                code = extract_code(response)
                print(f"--- ä¿®æ­£åçš„ä»£ç  ---\n{code}\n------------------")
        
        # --- æ­¥éª¤ C: è§£é‡Šç»“æœ ---
        if execution_success:
            print("\nğŸ“ æ­£åœ¨ç”Ÿæˆå›ç­”...")
            final_prompt = get_explanation_prompt(user_query, execution_output)
            final_answer = llm.get_response(final_prompt)
            print(f"\nğŸ¤– Agent å›ç­”:\n{final_answer}")
        else:
            print("\nğŸ’€ ä»»åŠ¡å¤±è´¥: ä»£ç ç»è¿‡å¤šæ¬¡ä¿®æ­£ä»æ— æ³•è¿è¡Œã€‚")

if __name__ == "__main__":
    main()